{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/root/dev/vcmr\")\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import functional as F\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "from train import MLP, MusicTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_features_dir = './music_features_test'\n",
    "music_samplecnn_dir = './music_SampleCNN_test'\n",
    "image_features_dir = './image_features_test'\n",
    "image_dir = './images_test'\n",
    "music_dir = './music_test'\n",
    "\n",
    "def compute_recall_at_k(similarities, ground_truth, k):\n",
    "    topk = similarities.topk(k, dim=1, largest=True, sorted=True).indices  # (num_queries, k)\n",
    "    recall = (topk == ground_truth.unsqueeze(1)).any(dim=1).float().mean().item()\n",
    "    return recall\n",
    "\n",
    "def compute_mrr(similarities, ground_truth):\n",
    "    ranks = (similarities.argsort(descending=True) == ground_truth.unsqueeze(1)).float()\n",
    "    ranks = ranks.nonzero(as_tuple=False)\n",
    "    ranks = ranks[ranks[:,1] == ground_truth[ranks[:,0]], 1] + 1  # 순위는 1부터 시작\n",
    "    reciprocal_ranks = 1.0 / ranks.float()\n",
    "    mrr = reciprocal_ranks.mean().item()\n",
    "    return mrr\n",
    "\n",
    "def compute_map(similarities, ground_truth):\n",
    "    sorted_indices = similarities.argsort(descending=True)  # (num_queries, num_targets)\n",
    "    num_queries = similarities.size(0)\n",
    "    average_precisions = []\n",
    "\n",
    "    for i in range(num_queries):\n",
    "        rank = (sorted_indices[i] == ground_truth[i]).nonzero(as_tuple=False)\n",
    "        if rank.numel() == 0:\n",
    "            average_precisions.append(0.0)\n",
    "        else:\n",
    "            rank = rank.item() + 1  # 순위는 1부터 시작\n",
    "            average_precisions.append(1.0 / rank)\n",
    "\n",
    "    map_score = np.mean(average_precisions)\n",
    "    return map_score\n",
    "\n",
    "def compute_median_rank(similarities, ground_truth):\n",
    "    sorted_indices = similarities.argsort(descending=True)  # (num_queries, num_targets)\n",
    "    num_queries = similarities.size(0)\n",
    "    ranks = []\n",
    "\n",
    "    for i in range(num_queries):\n",
    "        rank = (sorted_indices[i] == ground_truth[i]).nonzero(as_tuple=False)\n",
    "        if rank.numel() == 0:\n",
    "            ranks.append(similarities.size(1) + 1)  # 정답이 없을 경우 최대 순위 +1\n",
    "        else:\n",
    "            ranks.append(rank.item() + 1)  # 순위는 1부터 시작\n",
    "\n",
    "    median_rank = np.median(ranks)\n",
    "    return median_rank\n",
    "\n",
    "class MusicImageDataset(Dataset):\n",
    "    def __init__(self, music_features_dir, image_features_dir, image_dir, is_transformer):\n",
    "        self.music_features_dir = music_features_dir\n",
    "        self.image_features_dir = image_features_dir\n",
    "        self.music_samplecnn_dir = music_samplecnn_dir\n",
    "        self.is_transformer = is_transformer\n",
    "        self.image_dir = image_dir\n",
    "\n",
    "        files = [os.path.splitext(f)[0] for f in os.listdir(music_features_dir) if f.endswith('.npy')]\n",
    "\n",
    "        if self.is_transformer:\n",
    "            sample_cnn_files = [os.path.splitext(f)[0]  for f in os.listdir(music_samplecnn_dir) if f.endswith('.npy')]\n",
    "            file_set = set(files).intersection(sample_cnn_files)\n",
    "            self.filenames = sorted(list(file_set))\n",
    "        else:\n",
    "            self.filenames = sorted(files)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.filenames[idx]\n",
    "        music_embed = np.load(os.path.join(self.music_features_dir, f\"{fname}.npy\")).astype(np.float32)\n",
    "        image_embed = np.load(os.path.join(self.image_features_dir, f\"{fname}.npy\")).astype(np.float32)\n",
    "        music_samplecnn_embed = music_embed\n",
    "        if self.is_transformer:\n",
    "            music_samplecnn_embed = np.load(os.path.join(self.music_samplecnn_dir, f\"{fname}.npy\")).astype(np.float32)\n",
    "        return image_embed, music_embed, music_samplecnn_embed, idx, fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(image_proj, music_proj, dataloader, device, is_transformer = False):\n",
    "    all_music_embeddings = []\n",
    "    all_fnames = []\n",
    "\n",
    "    print(\"음악 임베딩 사전 계산 중...\")\n",
    "    with torch.no_grad():\n",
    "        for image_embed, music_embed, music_samplecnn_embed, idx, fname in tqdm(dataloader, desc=\"음악 임베딩 계산\"):\n",
    "            music_embed = torch.Tensor(music_embed).to(device)  # (batch_size, 50)\n",
    "            music_samplecnn_embed = torch.Tensor(music_samplecnn_embed).to(device)  # (batch_size, 50)\n",
    "            if is_transformer:\n",
    "                projected_music = music_proj(music_embed, music_samplecnn_embed)  # (batch_size, 128)\n",
    "            else:\n",
    "                projected_music = music_proj(music_embed)\n",
    "            all_music_embeddings.append(projected_music.cpu())\n",
    "            all_fnames.extend(fname)\n",
    "    all_music_embeddings = torch.cat(all_music_embeddings, dim=0)  # (num_music, 128)\n",
    "    all_music_embeddings = F.normalize(all_music_embeddings, dim=1)  # 정규화\n",
    "\n",
    "    print(\"이미지 임베딩 사전 계산 중...\")\n",
    "    image_embeddings = []\n",
    "    ground_truth = []\n",
    "    with torch.no_grad():\n",
    "        for image_embed, music_embed, music_samplecnn_embed, idx, fname in tqdm(dataloader, desc=\"이미지 임베딩 계산\"):\n",
    "            image_embed = torch.Tensor(image_embed).to(device)  # (batch_size, 512)\n",
    "            projected_image = image_proj(image_embed)  # (batch_size, 128)\n",
    "            image_embeddings.append(projected_image.cpu())\n",
    "            ground_truth.extend([i for i in range(len(fname))])\n",
    "    image_embeddings = torch.cat(image_embeddings, dim=0)  # (num_images, 128)\n",
    "    image_embeddings = F.normalize(image_embeddings, dim=1)  # 정규화\n",
    "\n",
    "    return image_embeddings, all_music_embeddings, all_fnames\n",
    "\n",
    "def evaluate_model(image_embeddings, music_embeddings, device, topk=[1,5,10]):\n",
    "    print(\"유사도 매트릭스 계산 중...\")\n",
    "    similarities = torch.mm(image_embeddings, music_embeddings.T)  # (num_images, num_music)\n",
    "    num_queries = similarities.size(0)\n",
    "    num_targets = similarities.size(1)\n",
    "    ground_truth_indices = torch.arange(num_queries).to(device)  # (num_queries,)\n",
    "    similarities = similarities.to(device)\n",
    "\n",
    "    print(\"평가 지표 계산 중...\")\n",
    "    metrics = {}\n",
    "    for k in topk:\n",
    "        metrics[f'Recall@{k}'] = compute_recall_at_k(similarities, ground_truth_indices, k)\n",
    "\n",
    "    # metrics[\"MRR\"] = compute_mrr(similarities, ground_truth_indices)\n",
    "    metrics[\"MR\"] = compute_median_rank(similarities, ground_truth_indices)\n",
    "    metrics[\"mAP\"] = compute_map(similarities, ground_truth_indices)\n",
    "\n",
    "    print(\"평가 완료!\")\n",
    "    return metrics\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(checkpoint_path):\n",
    "    def is_transformer(checkpoint_path):\n",
    "        return \"trans\" in checkpoint_path\n",
    "    pth_dir = checkpoint_path\n",
    "    __is_transformer = False\n",
    "\n",
    "    checkpoint = torch.load(pth_dir)\n",
    "    image_proj_state_dict = checkpoint['image_proj_state_dict']\n",
    "    music_proj_state_dict = checkpoint['music_proj_state_dict']\n",
    "\n",
    "    image_proj = MLP(input_dim=512, output_dim=256)\n",
    "    image_proj.load_state_dict(image_proj_state_dict)\n",
    "    image_proj.cuda().eval()\n",
    "\n",
    "    if is_transformer(checkpoint_path):\n",
    "        music_proj = MusicTransformer(input_dim=512, output_dim=256)\n",
    "        __is_transformer = True\n",
    "    else:\n",
    "        music_proj = MLP(input_dim=512, output_dim=256)\n",
    "    music_proj.load_state_dict(music_proj_state_dict)\n",
    "    music_proj.cuda().eval()\n",
    "\n",
    "    return image_proj, music_proj, __is_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_proj, music_proj, is_transformer = get_model('./checkpoint_epoch_200_infonce.pth')\n",
    "dataset = MusicImageDataset(music_features_dir, image_features_dir, image_dir, is_transformer)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "image_embeddings, all_music_embeddings, all_fnames = get_embeddings(image_proj, music_proj, dataloader, device, is_transformer)\n",
    "evaluate_model(image_embeddings, all_music_embeddings, device, topk=[1,5,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "def visualize_recommendations(fnames, image_proj, dataloader, device, idx, music_embeddings, image_dir, topk=10, quantile=20):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_embed, _, _, _, fname = dataloader.dataset[idx]\n",
    "        image_embed = torch.Tensor(image_embed).unsqueeze(0).to(device)  # (1, 512)\n",
    "        projected_image = image_proj(image_embed)  # (1, 128)\n",
    "        projected_image = F.normalize(projected_image, dim=1)  # 정규화\n",
    "\n",
    "        length = len(music_embeddings) #9093\n",
    "        similarities = torch.mm(projected_image, music_embeddings.to(device).T)  # (1, num_music)\n",
    "        topk_similarities, topk_indices = similarities.topk(length, dim=1, largest=True, sorted=True)\n",
    "\n",
    "        topk_similarities = topk_similarities.squeeze(0).cpu().numpy()\n",
    "        topk_indices = topk_indices.squeeze(0).cpu().numpy()\n",
    "\n",
    "        gt_fname = os.path.splitext(fname)[0]\n",
    "        gt_idx = fnames.index(gt_fname)\n",
    "\n",
    "        image_path = os.path.join(image_dir, f\"{gt_fname}.jpg\")\n",
    "        image = Image.open(image_path)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')  # 축 숨기기\n",
    "        plt.title(f\"{gt_fname}\")\n",
    "        plt.show()\n",
    "        print(f\"정답 음악: {gt_fname}\")\n",
    "\n",
    "        for rank, (sim, idx) in enumerate(zip(topk_similarities, topk_indices), start=1):\n",
    "            music_fname = fnames[idx]\n",
    "            if idx == gt_idx:\n",
    "                print(f\"{rank}. {music_fname}: {sim:.4f} [GT]\")\n",
    "        display(Audio(os.path.join(music_dir, f\"{gt_fname}.mp3\")))\n",
    "\n",
    "        print(\"TopK\")\n",
    "\n",
    "        for rank, (sim, idx) in enumerate(zip(topk_similarities, topk_indices), start=1):\n",
    "            if rank > topk:\n",
    "                break\n",
    "            music_fname = fnames[idx]\n",
    "            print(f\"{rank}. {music_fname}: {sim:.4f}\")\n",
    "            audio = Audio(os.path.join(music_dir, music_fname + \".mp3\"))\n",
    "            display(audio)\n",
    "\n",
    "        print(\"Quantile\")\n",
    "        quantiles = []\n",
    "        for j in range(0, length, max(1, length // quantile)):\n",
    "            quantiles.append(topk_similarities[j])\n",
    "        print(\"\\t\".join([f\"{q:.3f}\" for q in quantiles]))\n",
    "idx = random.randint(0, len(dataset))\n",
    "visualize_recommendations(all_fnames, image_proj, dataloader, device, idx, all_music_embeddings, image_dir, topk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COSINE SIMILARITY (Positive:Negative=1:1)\n",
    "{'Recall@1': 0.00021994940470904112,\n",
    " 'Recall@5': 0.000989772379398346,\n",
    " 'Recall@10': 0.0016496204771101475,\n",
    " 'MR': np.float64(3914.0),\n",
    " 'mAP': np.float64(0.001565276893921723)}\n",
    "\n",
    "# COSINE SIMILARITY (Positive:Negative=1:3)\n",
    "{'Recall@1': 0.0003299241070635617,\n",
    " 'Recall@5': 0.0015396458329632878,\n",
    " 'Recall@10': 0.0024194433353841305,\n",
    " 'MR': np.float64(4223.0),\n",
    " 'mAP': np.float64(0.0019648388151269503)}\n",
    "\n",
    "# InfoNCE\n",
    "{'Recall@1': 0.00043989880941808224,\n",
    " 'Recall@5': 0.001759595237672329,\n",
    " 'Recall@10': 0.003519190475344658,\n",
    " 'MR': np.float64(2966.0),\n",
    " 'mAP': np.float64(0.0025402929552953454)}\n",
    "\n",
    "# 아래부터는 안 돠도 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing the testset (6.0s for 10086 files)\n",
    "\n",
    "music_features_dir = './music_features_test'\n",
    "image_features_dir = './image_features_test'\n",
    "image_dir = './images_test'\n",
    "\n",
    "files = sorted([f for f in os.listdir(music_features_dir) if f.endswith('.npy')])\n",
    "filenames = [os.path.splitext(f)[0] for f in files]\n",
    "music_embeddings = {}\n",
    "for f in files:\n",
    "    stem = os.path.splitext(f)[0]\n",
    "    # music_embeddings[stem] = np.load(os.path.join(music_features_dir, f)).astype(np.float32)\n",
    "    music_embed = torch.Tensor(music_embed).unsqueeze(0).cuda()\n",
    "    with torch.no_grad():\n",
    "        music_embed_output = model(music_embed)\n",
    "    music_embeddings[stem] = music_embed_output.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기부터 안봐도됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "base_fname = files[idx]\n",
    "image_embed = np.load(os.path.join(image_features_dir, base_fname)).astype(np.float32)\n",
    "image_embed = torch.Tensor(image_embed).unsqueeze(0).cuda()\n",
    "with torch.no_grad():\n",
    "    image_embed_output = model(image_embed)\n",
    "\n",
    "similarities = []\n",
    "for fname, music_embed in music_embeddings.items():\n",
    "    sim = F.cosine_similarity(image_embed_output, torch.Tensor(music_embed).cuda()).item()\n",
    "    similarities.append((fname, sim))\n",
    "    if fname == base_fname.split('.')[0]:\n",
    "        print(f\"{fname}: {sim:.4f} - GT\")\n",
    "top_10_similarities = sorted(similarities, key=lambda x: x[1], reverse=True)[:20]\n",
    "for fname, sim in top_10_similarities:\n",
    "    print(f\"{fname}: {sim:.4f}\")\n",
    "\n",
    "image_path = f\"./{image_dir}/{base_fname[:-4]}.jpg\"\n",
    "image = Image.open(image_path)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # Hide axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_embed = music_embeddings[\"LgJhcGl_h7Y_2\"]\n",
    "\n",
    "image_embed_outputs = []\n",
    "for idx in range(0, 9093):\n",
    "    image_embed = np.load(os.path.join(image_features_dir, files[idx])).astype(np.float32)\n",
    "    image_embed = torch.Tensor(image_embed).unsqueeze(0).cuda()\n",
    "    with torch.no_grad():\n",
    "        image_embed_output = model(image_embed)\n",
    "    image_embed_outputs.append(image_embed_output)\n",
    "\n",
    "similarities = []\n",
    "for j, image_embed_output in enumerate(image_embed_outputs):\n",
    "    sim = F.cosine_similarity(image_embed_output, torch.Tensor(music_embed).cuda()).item()\n",
    "    similarities.append((files[j], sim))\n",
    "\n",
    "top_30_similarities = sorted(similarities, key=lambda x: x[1], reverse=True)[:20]\n",
    "for fname, sim in top_30_similarities:\n",
    "    print(f\"{fname}: {sim:.4f}\")\n",
    "\n",
    "low_30_similarities = sorted(similarities, key=lambda x: x[1])[:20]\n",
    "for fname, sim in low_30_similarities:\n",
    "    print(f\"{fname}: {sim:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = \"DRlptgSj9qM_2\"\n",
    "music_embed = music_embeddings[l]\n",
    "\n",
    "similarities = []\n",
    "for j, image_embed_output in enumerate(image_embed_outputs):\n",
    "    sim = F.cosine_similarity(image_embed_output, torch.Tensor(music_embed).cuda()).item()\n",
    "    similarities.append((files[j], sim))\n",
    "\n",
    "top_30_similarities = sorted(similarities, key=lambda x: x[1], reverse=True)[:20]\n",
    "for fname, sim in top_30_similarities:\n",
    "    print(f\"{fname}: {sim:.4f}\")\n",
    "\n",
    "low_30_similarities = sorted(similarities, key=lambda x: x[1])[:20]\n",
    "for fname, sim in low_30_similarities:\n",
    "    print(f\"{fname}: {sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_10_similarities = sorted(similarities, key=lambda x: x[1])[:20]\n",
    "for fname, sim in lowest_10_similarities:\n",
    "    print(f\"{fname}: {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(9093):\n",
    "\n",
    "    base_fname = files[idx]\n",
    "    image_embed = np.load(os.path.join(image_features_dir, base_fname)).astype(np.float32)\n",
    "    image_embed = torch.Tensor(image_embed).unsqueeze(0).cuda()\n",
    "    with torch.no_grad():\n",
    "        image_embed_output = model(image_embed)\n",
    "    music_embed = music_embeddings[base_fname.split('.')[0]]\n",
    "    music_embed = torch.Tensor(music_embed).unsqueeze(0).cuda()\n",
    "    sim = F.cosine_similarity(image_embed_output, music_embed).item()\n",
    "    print(f\"{base_fname}: {sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "l = []\n",
    "for i in range(600, 700):\n",
    "    music_embed_1 = music_embeddings[files[i].split('.')[0]]\n",
    "    music_embed_1 = torch.Tensor(music_embed_1).unsqueeze(0)\n",
    "    for j in range(i + 1, 9093):\n",
    "        music_embed_2 = music_embeddings[files[j].split('.')[0]]\n",
    "        music_embed_2 = torch.Tensor(music_embed_2).unsqueeze(0)\n",
    "        sim = F.cosine_similarity(music_embed_1, music_embed_2).item()\n",
    "        l.append((i, j, sim))\n",
    "\n",
    "lowest_20_similarities = sorted(l, key=lambda x: x[2])[:20]\n",
    "for i, k, sim in lowest_20_similarities:\n",
    "    print(f\"({i}, {k}): {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
